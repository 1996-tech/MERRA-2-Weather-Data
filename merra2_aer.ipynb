{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a2c6a-114c-4232-9e16-9ddfd50899c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def process_multiple_merra2_files_washington(directory_path):\n",
    "    # Washington state boundaries\n",
    "    WA_LAT_MIN, WA_LAT_MAX = 45.5, 49.0\n",
    "    WA_LON_MIN, WA_LON_MAX = -124.8, -116.9\n",
    "    \n",
    "    # Get all MERRA2 files in the directory\n",
    "    file_pattern = os.path.join(directory_path, \"MERRA2_400.tavg1_2d_aer_Nx.*\")\n",
    "    files = sorted(glob.glob(file_pattern))\n",
    "    \n",
    "    print(f\"Found {len(files)} files to process\")\n",
    "    \n",
    "    # Lists to store data frames and their columns\n",
    "    hourly_data_frames = []\n",
    "    daily_data_frames = []\n",
    "    \n",
    "    # Process first file to get coordinates\n",
    "    first_file = files[0]\n",
    "    with xr.open_dataset(first_file) as ds:\n",
    "        lats = ds['lat'].values\n",
    "        lons = ds['lon'].values\n",
    "        \n",
    "        # Filter coordinates for Washington state\n",
    "        wa_lat_mask = (lats >= WA_LAT_MIN) & (lats <= WA_LAT_MAX)\n",
    "        wa_lon_mask = (lons >= WA_LON_MIN) & (lons <= WA_LON_MAX)\n",
    "        \n",
    "        wa_lats = lats[wa_lat_mask]\n",
    "        wa_lons = lons[wa_lon_mask]\n",
    "        \n",
    "        # Create base coordinate DataFrame\n",
    "        coords = [[lat, lon] for lat in wa_lats for lon in wa_lons]\n",
    "        base_df = pd.DataFrame(coords, columns=['latitude', 'longitude'])\n",
    "    \n",
    "    # Process all files\n",
    "    for idx, file in enumerate(files, 1):\n",
    "        print(f\"Processing file {idx}/{365}: {os.path.basename(file)}\", end='\\r')\n",
    "        \n",
    "        try:\n",
    "            # Open the NetCDF file using xarray\n",
    "            ds = xr.open_dataset(file)\n",
    "            \n",
    "            # Extract TOTANGSTR values for Washington only\n",
    "            totangstr = ds['TOTANGSTR'].values[:, wa_lat_mask, :][:, :, wa_lon_mask]\n",
    "            \n",
    "            # Extract time and convert to UTC\n",
    "            times = pd.to_datetime(ds['time'].values)\n",
    "            \n",
    "            # Create DataFrames for this file's data\n",
    "            hourly_data = {}\n",
    "            \n",
    "            # Process each timestamp\n",
    "            for t_idx, timestamp in enumerate(times):\n",
    "                time_data = []\n",
    "                for lat_idx, lat in enumerate(wa_lats):\n",
    "                    for lon_idx, lon in enumerate(wa_lons):\n",
    "                        time_data.append(totangstr[t_idx, lat_idx, lon_idx])\n",
    "                hourly_data[timestamp] = time_data\n",
    "            \n",
    "            # Create hourly DataFrame for this file\n",
    "            hourly_df = pd.DataFrame(hourly_data)\n",
    "            \n",
    "            # Calculate daily averages (fixed deprecation warning)\n",
    "            daily_df = hourly_df.T.groupby(lambda x: x.date()).mean().T\n",
    "            \n",
    "            # Append to lists\n",
    "            hourly_data_frames.append(hourly_df)\n",
    "            daily_data_frames.append(daily_df)\n",
    "            \n",
    "            ds.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing file {file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\nCombining all data...\")\n",
    "    \n",
    "    # Combine all hourly data\n",
    "    combined_hourly = pd.concat(hourly_data_frames, axis=1)\n",
    "    combined_hourly = pd.concat([base_df, combined_hourly], axis=1)\n",
    "    \n",
    "    # Combine all daily data\n",
    "    combined_daily = pd.concat(daily_data_frames, axis=1)\n",
    "    combined_daily = pd.concat([base_df, combined_daily], axis=1)\n",
    "    \n",
    "    # Sort columns (except first two lat/lon columns)\n",
    "    time_cols_hourly = combined_hourly.columns[2:]\n",
    "    time_cols_daily = combined_daily.columns[2:]\n",
    "    \n",
    "    combined_hourly = pd.concat([\n",
    "        combined_hourly.iloc[:, :2],\n",
    "        combined_hourly[sorted(time_cols_hourly)]\n",
    "    ], axis=1)\n",
    "    \n",
    "    combined_daily = pd.concat([\n",
    "        combined_daily.iloc[:, :2],\n",
    "        combined_daily[sorted(time_cols_daily)]\n",
    "    ], axis=1)\n",
    "    \n",
    "    return combined_hourly, combined_daily\n",
    "\n",
    "# Usage example\n",
    "directory_path = \"Downloads/M2T1NXAER_5.12.4-20250117_084759\"  # Replace with your directory path\n",
    "hourly_df, daily_df = process_multiple_merra2_files_washington(directory_path)\n",
    "\n",
    "# Save to CSV\n",
    "print(\"Saving results to CSV files...\")\n",
    "hourly_df.to_csv(\"washington_merra2_hourly_data.csv\", index=False)\n",
    "daily_df.to_csv(\"washington_merra2_daily_data.csv\", index=False)\n",
    "\n",
    "# Print information about the results\n",
    "print(\"\\nHourly data shape:\", hourly_df.shape)\n",
    "print(\"Daily data shape:\", daily_df.shape)\n",
    "print(\"\\nNumber of grid points:\", len(hourly_df))\n",
    "print(\"\\nSample of hourly data:\")\n",
    "print(hourly_df.head())\n",
    "print(\"\\nSample of daily data:\")\n",
    "print(daily_df.head())\n",
    "\n",
    "# Print coordinate ranges to verify\n",
    "print(\"\\nLatitude range:\", hourly_df['latitude'].min(), \"to\", hourly_df['latitude'].max())\n",
    "print(\"Longitude range:\", hourly_df['longitude'].min(), \"to\", hourly_df['longitude'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efdcc77-9835-47bf-bae1-de5b0b99f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def process_multiple_merra2_files_washington(directory_path):\n",
    "    # Washington state boundaries\n",
    "    WA_LAT_MIN, WA_LAT_MAX = 45.5, 49.0\n",
    "    WA_LON_MIN, WA_LON_MAX = -124.8, -116.9\n",
    "    \n",
    "    # Get all MERRA2 files in the directory\n",
    "    file_pattern = os.path.join(directory_path, \"MERRA2_400.tavg1_2d_aer_Nx.*\")\n",
    "    files = sorted(glob.glob(file_pattern))\n",
    "    \n",
    "    print(f\"Found {len(files)} files to process\")\n",
    "    \n",
    "    # Lists to store DataFrames\n",
    "    hourly_dfs = []\n",
    "    daily_dfs = []\n",
    "    \n",
    "    for idx, file in enumerate(files, 1):\n",
    "        print(f\"Processing file {idx}/{len(files)}: {os.path.basename(file)}\")\n",
    "        \n",
    "        try:\n",
    "            # Open the NetCDF file using xarray\n",
    "            ds = xr.open_dataset(file)\n",
    "            \n",
    "            # Extract latitude and longitude\n",
    "            lats = ds['lat'].values\n",
    "            lons = ds['lon'].values\n",
    "            \n",
    "            # Filter coordinates for Washington state\n",
    "            wa_lat_mask = (lats >= WA_LAT_MIN) & (lats <= WA_LAT_MAX)\n",
    "            wa_lon_mask = (lons >= WA_LON_MIN) & (lons <= WA_LON_MAX)\n",
    "            \n",
    "            wa_lats = lats[wa_lat_mask]\n",
    "            wa_lons = lons[wa_lon_mask]\n",
    "            \n",
    "            # Extract TOTANGSTR values for Washington only\n",
    "            BCCMASS = ds['BCCMASS'].values[:, wa_lat_mask, :][:, :, wa_lon_mask]\n",
    "            \n",
    "            # Extract time and convert to UTC\n",
    "            times = ds['time'].values\n",
    "            time_utc = pd.to_datetime(times)\n",
    "            \n",
    "            # Create a list for Washington coordinates\n",
    "            coords = []\n",
    "            for lat in wa_lats:\n",
    "                for lon in wa_lons:\n",
    "                    coords.append([lat, lon])\n",
    "            \n",
    "            # Create a DataFrame with coordinates as index\n",
    "            df = pd.DataFrame(index=pd.MultiIndex.from_tuples(coords, names=['latitude', 'longitude']))\n",
    "            \n",
    "            # Add time data\n",
    "            for t_idx, timestamp in enumerate(time_utc):\n",
    "                time_data = []\n",
    "                for lat_idx, lat in enumerate(wa_lats):\n",
    "                    for lon_idx, lon in enumerate(wa_lons):\n",
    "                        time_data.append(BCCMASS[t_idx, lat_idx, lon_idx])\n",
    "                df[timestamp] = time_data\n",
    "            \n",
    "            # Reset index to convert MultiIndex to columns\n",
    "            df = df.reset_index()\n",
    "            \n",
    "            # Calculate daily averages\n",
    "            time_columns = df.columns[2:]  # Skip latitude and longitude columns\n",
    "            daily_df = pd.DataFrame()\n",
    "            daily_df['latitude'] = df['latitude']\n",
    "            daily_df['longitude'] = df['longitude']\n",
    "            \n",
    "            # Group timestamps by date and calculate mean\n",
    "            dates = pd.to_datetime(time_columns).date\n",
    "            unique_dates = list(set(dates))\n",
    "            for date in unique_dates:\n",
    "                date_cols = [col for col, d in zip(time_columns, dates) if d == date]\n",
    "                daily_df[date] = df[date_cols].mean(axis=1)\n",
    "            \n",
    "            # Append to lists\n",
    "            hourly_dfs.append(df)\n",
    "            daily_dfs.append(daily_df)\n",
    "            \n",
    "            # Close the dataset\n",
    "            ds.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    print(\"Combining all data...\")\n",
    "    \n",
    "    # For hourly data\n",
    "    combined_hourly = hourly_dfs[0].copy()\n",
    "    for df in hourly_dfs[1:]:\n",
    "        # Only append the time columns, not lat/lon\n",
    "        new_columns = df.columns[2:]\n",
    "        combined_hourly[new_columns] = df[new_columns]\n",
    "    \n",
    "    # For daily data\n",
    "    combined_daily = daily_dfs[0].copy()\n",
    "    for df in daily_dfs[1:]:\n",
    "        # Only append the date columns, not lat/lon\n",
    "        new_columns = df.columns[2:]\n",
    "        combined_daily[new_columns] = df[new_columns]\n",
    "    \n",
    "    # Sort columns (except first two lat/lon columns)\n",
    "    combined_hourly = pd.concat([\n",
    "        combined_hourly.iloc[:, :2],\n",
    "        combined_hourly.iloc[:, 2:].sort_index(axis=1)\n",
    "    ], axis=1)\n",
    "    \n",
    "    combined_daily = pd.concat([\n",
    "        combined_daily.iloc[:, :2],\n",
    "        combined_daily.iloc[:, 2:].sort_index(axis=1)\n",
    "    ], axis=1)\n",
    "    \n",
    "    return combined_hourly, combined_daily\n",
    "\n",
    "# Usage example\n",
    "directory_path = \"Downloads/M2T1NXAER_5.12.4-20250117_084759\"  # Replace with your directory path\n",
    "hourly_df, daily_df = process_multiple_merra2_files_washington(directory_path)\n",
    "\n",
    "# Save to CSV\n",
    "print(\"Saving results to CSV files...\")\n",
    "hourly_df.to_csv(\"washington_merra2_bcmassdensity_hourly_data.csv\", index=False)\n",
    "daily_df.to_csv(\"washington_merra2_bcmassdensity_daily_data.csv\", index=False)\n",
    "\n",
    "# Print information about the results\n",
    "print(\"\\nHourly data shape:\", hourly_df.shape)\n",
    "print(\"Daily data shape:\", daily_df.shape)\n",
    "print(\"\\nNumber of grid points:\", len(hourly_df))\n",
    "print(\"\\nSample of hourly data:\")\n",
    "print(hourly_df.head())\n",
    "print(\"\\nSample of daily data:\")\n",
    "print(daily_df.head())\n",
    "\n",
    "# Print coordinate ranges to verify\n",
    "print(\"\\nLatitude range:\", hourly_df['latitude'].min(), \"to\", hourly_df['latitude'].max())\n",
    "print(\"Longitude range:\", hourly_df['longitude'].min(), \"to\", hourly_df['longitude'].max())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
